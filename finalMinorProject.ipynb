{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2abb8dd-4c37-410d-b83f-8f0e4eb38bc4",
   "metadata": {},
   "source": [
    "# Speech to Control Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b21c68f-f020-454e-a390-ded2e3ee8a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: write direction\n",
      "Triggering 'right' key for 'write'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: down direction\n",
      "Triggering 'down' key for 'down'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: left direction\n",
      "Triggering 'left' key for 'left'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: down direction\n",
      "Triggering 'down' key for 'down'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: write direction\n",
      "Triggering 'right' key for 'write'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: right direction\n",
      "Triggering 'right' key for 'right'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: town direction\n",
      "Triggering 'down' key for 'town'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: left direction\n",
      "Triggering 'left' key for 'left'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: town direction\n",
      "Triggering 'down' key for 'town'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: right direction\n",
      "Triggering 'right' key for 'right'\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: left direction\n",
      "Triggering 'left' key for 'left'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: town direction\n",
      "Triggering 'down' key for 'town'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: write direction\n",
      "Triggering 'right' key for 'write'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: town direction\n",
      "Triggering 'down' key for 'town'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: left direction\n",
      "Triggering 'left' key for 'left'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: write direction\n",
      "Triggering 'right' key for 'write'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: up direction\n",
      "Triggering 'up' key for 'up'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: left direction\n",
      "Triggering 'left' key for 'left'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: town direction\n",
      "Triggering 'down' key for 'town'\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: town direction\n",
      "Triggering 'down' key for 'town'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: right direction\n",
      "Triggering 'right' key for 'right'\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Could not understand audio. Please try again.\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: town direction\n",
      "Triggering 'down' key for 'town'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: right direction\n",
      "Triggering 'right' key for 'right'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: left direction\n",
      "Triggering 'left' key for 'left'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: left direction\n",
      "Triggering 'left' key for 'left'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: town direction\n",
      "Triggering 'down' key for 'town'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: light direction\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: town direction\n",
      "Triggering 'down' key for 'town'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: left direction\n",
      "Triggering 'left' key for 'left'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: right direction\n",
      "Triggering 'right' key for 'right'\n",
      "Listening... Please say a direction or keyword...\n",
      "Recognized text: please stop\n",
      "Stopping the program as 'please stop' was detected.\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import pyautogui\n",
    "import threading\n",
    "import time\n",
    "\n",
    "r = sr.Recognizer()\n",
    "pyautogui.PAUSE = 0.05\n",
    "\n",
    "keywords = {\n",
    "    \"left\": \"left\",\n",
    "    \"right\": \"right\",\n",
    "    \"down\": \"down\",\n",
    "    \"up\": \"up\",\n",
    "    \"write\": \"right\",\n",
    "    \"town\": \"down\",\n",
    "    \"aap\": \"up\",\n",
    "}\n",
    "\n",
    "def record_text():\n",
    "    while True:\n",
    "        try:\n",
    "            with sr.Microphone() as source2:\n",
    "                r.adjust_for_ambient_noise(source2, duration=0.04)\n",
    "                print(\"Listening... Please say a direction or keyword...\")\n",
    "                audio2 = r.listen(source2, phrase_time_limit=1.5)\n",
    "                myText = r.recognize_google(audio2, language=\"en-US\").lower()\n",
    "                print(f\"Recognized text: {myText}\")\n",
    "                return myText\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio. Please try again.\")\n",
    "            time.sleep(0.5)\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results: {e}\")\n",
    "            return None\n",
    "\n",
    "def process_and_trigger_keypress(text):\n",
    "    word_set = set(text.split())\n",
    "    for word in keywords.keys() & word_set:\n",
    "        action = keywords[word]\n",
    "        print(f\"Triggering '{action}' key for '{word}'\")\n",
    "        pyautogui.press(action)\n",
    "\n",
    "def process_in_thread(detected_text):\n",
    "    thread = threading.Thread(target=process_and_trigger_keypress, args=(detected_text,))\n",
    "    thread.start()\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        detected_text = record_text()\n",
    "        if detected_text:\n",
    "            if \"please stop\" in detected_text:\n",
    "                print(\"Stopping the program as 'please stop' was detected.\")\n",
    "                break\n",
    "            process_in_thread(detected_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5731b1ec-3dc1-4116-b15e-bacd1fb40085",
   "metadata": {},
   "source": [
    "# Head to Gesture Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d8def6-3700-43eb-bf45-5c261d190135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def get_nose_coordinates(face_landmarks, image_shape):\n",
    "    if face_landmarks:\n",
    "        height, width, _ = image_shape\n",
    "        nose_tip = face_landmarks.landmark[1]\n",
    "        x = int(nose_tip.x * width)\n",
    "        y = int(nose_tip.y * height)\n",
    "        return x, y\n",
    "    return None, None\n",
    "\n",
    "def get_lips_distance(face_landmarks, image_shape):\n",
    "    if face_landmarks:\n",
    "        height, width, _ = image_shape\n",
    "        upper_lip = face_landmarks.landmark[13]  # Upper lip\n",
    "        lower_lip = face_landmarks.landmark[14]  # Lower lip\n",
    "        # Calculate the Euclidean distance between the upper and lower lips\n",
    "        lip_distance = ((upper_lip.x - lower_lip.x) ** 2 + (upper_lip.y - lower_lip.y) ** 2) ** 0.5\n",
    "        lip_distance_pixels = int(lip_distance * height)  # Convert to pixel distance\n",
    "        return lip_distance_pixels\n",
    "    return 0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    reference_position = None\n",
    "    start_time = time.time()\n",
    "    initialized = False\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        if results.face_landmarks:\n",
    "            nose_x, nose_y = get_nose_coordinates(results.face_landmarks, frame.shape)\n",
    "\n",
    "            if not initialized:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                cv2.putText(frame, f\"Initializing... {5 - int(elapsed_time)} sec\",\n",
    "                            (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                if elapsed_time >= 5:\n",
    "                    reference_position = (nose_x, nose_y)\n",
    "                    initialized = True\n",
    "                else:\n",
    "                    cv2.imshow(\"Head Movement Detection\", frame)\n",
    "                    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "            if reference_position:\n",
    "                ref_x, ref_y = reference_position\n",
    "                dx = nose_x - ref_x\n",
    "                dy = nose_y - ref_y\n",
    "\n",
    "                direction = None\n",
    "                if dy < -50:\n",
    "                    direction = \"Up\"\n",
    "                elif dy > 50:\n",
    "                    direction = \"Down\"\n",
    "                elif dx < -50:  # Reversed from dx > 50\n",
    "                    direction = \"Right\"\n",
    "                elif dx > 50:  # Reversed from dx < -50\n",
    "                    direction = \"Left\"\n",
    "\n",
    "                if direction:\n",
    "                    cv2.putText(frame, f\"Head Movement: {direction}\", (10, 100),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Mouth opening detection\n",
    "            lip_distance = get_lips_distance(results.face_landmarks, frame.shape)\n",
    "            if lip_distance > 20:  # Threshold for mouth opening\n",
    "                cv2.putText(frame, \"Gesture: Space\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "            cv2.circle(frame, (nose_x, nose_y), 5, (0, 255, 0), -1)\n",
    "            cv2.putText(frame, f\"Nose Tip: ({nose_x}, {nose_y})\", (nose_x + 10, nose_y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Head Movement and Space Gesture Detection\", frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a95bc78-a49e-43c1-9a7c-e6be7294a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mpq\n",
    "import time\n",
    "import pyautogui\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def get_nose_coordinates(face_landmarks, image_shape):\n",
    "    if face_landmarks:\n",
    "        height, width, _ = image_shape\n",
    "        nose_tip = face_landmarks.landmark[1]\n",
    "        x = int(nose_tip.x * width)\n",
    "        y = int(nose_tip.y * height)\n",
    "        return x, y\n",
    "    return None, None\n",
    "\n",
    "def get_lips_distance(face_landmarks, image_shape):\n",
    "    if face_landmarks:\n",
    "        height, width, _ = image_shape\n",
    "        upper_lip = face_landmarks.landmark[13]\n",
    "        lower_lip = face_landmarks.landmark[14]\n",
    "        lip_distance = ((upper_lip.x - lower_lip.x) ** 2 + (upper_lip.y - lower_lip.y) ** 2) ** 0.5\n",
    "        lip_distance_pixels = int(lip_distance * height)\n",
    "        return lip_distance_pixels\n",
    "    return 0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    reference_position = None\n",
    "    start_time = time.time()\n",
    "    initialized = False\n",
    "\n",
    "    # Flags to detect if the key was already pressed for a gesture\n",
    "    key_pressed = {'up': False, 'down': False, 'left': False, 'right': False, 'space': False}\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        if results.face_landmarks:\n",
    "            nose_x, nose_y = get_nose_coordinates(results.face_landmarks, frame.shape)\n",
    "\n",
    "            if not initialized:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                cv2.putText(frame, f\"Initializing... {5 - int(elapsed_time)} sec\",\n",
    "                            (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                if elapsed_time >= 5:\n",
    "                    reference_position = (nose_x, nose_y)\n",
    "                    initialized = True\n",
    "                else:\n",
    "                    cv2.imshow(\"Head Movement Detection\", frame)\n",
    "                    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "            if reference_position:\n",
    "                ref_x, ref_y = reference_position\n",
    "                dx = nose_x - ref_x\n",
    "                dy = nose_y - ref_y\n",
    "\n",
    "                direction = None\n",
    "                if dy < -50 and not key_pressed['up']:\n",
    "                    direction = \"Up\"\n",
    "                    pyautogui.press('up')\n",
    "                    key_pressed['up'] = True\n",
    "                elif dy > 50 and not key_pressed['down']:\n",
    "                    direction = \"Down\"\n",
    "                    pyautogui.press('down')\n",
    "                    key_pressed['down'] = True\n",
    "                elif dx > 50 and not key_pressed['left']:\n",
    "                    direction = \"Left\"\n",
    "                    pyautogui.press('left')\n",
    "                    key_pressed['left'] = True\n",
    "                elif dx < -50 and not key_pressed['right']:\n",
    "                    direction = \"Right\"\n",
    "                    pyautogui.press('right')\n",
    "                    key_pressed['right'] = True\n",
    "\n",
    "                if direction:\n",
    "                    cv2.putText(frame, f\"Head Movement: {direction}\", (10, 100),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            lip_distance = get_lips_distance(results.face_landmarks, frame.shape)\n",
    "\n",
    "            if lip_distance > 20 and not key_pressed['space']:\n",
    "                cv2.putText(frame, \"Space\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                pyautogui.press('space')\n",
    "                key_pressed['space'] = True\n",
    "\n",
    "            cv2.circle(frame, (nose_x, nose_y), 5, (0, 255, 0), -1)\n",
    "            cv2.putText(frame, f\"Nose Tip: ({nose_x}, {nose_y})\", (nose_x + 10, nose_y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Reset key pressed flags when no gesture is detected\n",
    "        if lip_distance <= 20:\n",
    "            key_pressed['space'] = False\n",
    "        if abs(dy) < 50 and abs(dx) < 50:\n",
    "            key_pressed = {key: False for key in key_pressed}  # Reset all movement flags\n",
    "\n",
    "        cv2.imshow(\"Head Movement Detection\", frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727dfe4-173d-4e75-959b-a9fc888ed3aa",
   "metadata": {},
   "source": [
    "# Hand to Control Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93cdae77-fba0-4367-bf0e-852df333ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def highlight_finger_points(image, hand_landmarks):\n",
    "    tips = [8, 12, 16, 20]\n",
    "    mcps = [5, 9, 13, 17]\n",
    "\n",
    "    if hand_landmarks:\n",
    "        for hand in hand_landmarks:\n",
    "            for idx in tips:\n",
    "                landmark = hand.landmark[idx]\n",
    "                h, w, _ = image.shape\n",
    "                cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "                cv2.circle(image, (cx, cy), 8, (0, 255, 0), -1)\n",
    "\n",
    "            for idx in mcps:\n",
    "                landmark = hand.landmark[idx]\n",
    "                h, w, _ = image.shape\n",
    "                cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "                cv2.circle(image, (cx, cy), 8, (0, 165, 255), -1)\n",
    "\n",
    "def check_conditions(results, image):\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks, hand_handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            h, w, _ = image.shape\n",
    "            fingertips = [hand_landmarks.landmark[i] for i in [8, 12, 16, 20]]\n",
    "            mcps = [hand_landmarks.landmark[i] for i in [5, 9, 13, 17]]\n",
    "\n",
    "            fingertip_y = [int(tip.y * h) for tip in fingertips]\n",
    "            mcp_y = [int(mcp.y * h) for mcp in mcps]\n",
    "\n",
    "            if all(fingertip_y[i] < mcp_y[i] for i in range(4)):\n",
    "                label = hand_handedness.classification[0].label\n",
    "                return label.lower()\n",
    "\n",
    "            elif fingertip_y[0] < mcp_y[0] and all(fingertip_y[i] >= mcp_y[i] for i in range(1, 4)):\n",
    "                return \"up\"\n",
    "\n",
    "            elif all(fingertip_y[i] > mcp_y[i] for i in range(4)):\n",
    "                return \"down\"\n",
    "\n",
    "            elif fingertip_y[3] < mcp_y[3]:\n",
    "                return \"space\"\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def display_text(image, text):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    position = (50, 50)\n",
    "    font_scale = 1\n",
    "    color = (0, 255, 0)\n",
    "    thickness = 2\n",
    "    cv2.putText(image, f\"Action: {text}\", position, font, font_scale, color, thickness)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break \n",
    "\n",
    "        image, results = mediapipe_detection(frame, hands)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            highlight_finger_points(image, results.multi_hand_landmarks)\n",
    "\n",
    "        text = check_conditions(results, image)\n",
    "             \n",
    "        if text == \"left\":\n",
    "            text = \"right\"\n",
    "        elif text == \"right\":\n",
    "            text = \"left\"\n",
    "\n",
    "        display_text(image, text)\n",
    "\n",
    "        cv2.imshow(\"Hand Gesture Detection\", image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5b6c2f9-bcc2-45b9-a732-f104a6a06b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time  # Import time module\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def highlight_finger_points(image, hand_landmarks):\n",
    "    tips = [8, 12, 16, 20]\n",
    "    mcps = [5, 9, 13, 17]\n",
    "\n",
    "    if hand_landmarks:\n",
    "        for hand in hand_landmarks:\n",
    "            for idx in tips:\n",
    "                landmark = hand.landmark[idx]\n",
    "                h, w, _ = image.shape\n",
    "                cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "                cv2.circle(image, (cx, cy), 8, (0, 255, 0), -1)\n",
    "\n",
    "            for idx in mcps:\n",
    "                landmark = hand.landmark[idx]\n",
    "                h, w, _ = image.shape\n",
    "                cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "                cv2.circle(image, (cx, cy), 8, (0, 165, 255), -1)\n",
    "\n",
    "def check_conditions(results, image):\n",
    "    \"\"\"\n",
    "    Detect hand gestures and their handedness.\n",
    "    \"\"\"\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks, hand_handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            h, w, _ = image.shape\n",
    "            fingertips = [hand_landmarks.landmark[i] for i in [8, 12, 16, 20]]\n",
    "            mcps = [hand_landmarks.landmark[i] for i in [5, 9, 13, 17]]\n",
    "\n",
    "            fingertip_y = [int(tip.y * h) for tip in fingertips]\n",
    "            mcp_y = [int(mcp.y * h) for mcp in mcps]\n",
    "\n",
    "            # Handedness: \"Left\" or \"Right\"\n",
    "            label = hand_handedness.classification[0].label.lower()\n",
    "\n",
    "            # All fingertips above their respective MCPs\n",
    "            if all(fingertip_y[i] < mcp_y[i] for i in range(4)):\n",
    "                return \"right\" if label == \"left\" else \"left\"  # Swap actions for left and right hands\n",
    "\n",
    "            # Only index finger is raised\n",
    "            elif fingertip_y[0] < mcp_y[0] and all(fingertip_y[i] >= mcp_y[i] for i in range(1, 4)):\n",
    "                return \"up\"\n",
    "\n",
    "            # All fingertips below their MCPs\n",
    "            elif all(fingertip_y[i] > mcp_y[i] for i in range(4)):\n",
    "                return \"down\"\n",
    "\n",
    "            # Pinky finger is raised\n",
    "            elif fingertip_y[3] < mcp_y[3]:\n",
    "                return \"space\"\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def trigger_key_action(action, last_action, last_action_time):\n",
    "    \"\"\"\n",
    "    Trigger a keyboard action based on the detected gesture with a delay.\n",
    "    \"\"\"\n",
    "    current_time = time.time()\n",
    "    \n",
    "    if action != last_action and (current_time - last_action_time >= 0.15):  # Only trigger if 0.15 seconds have passed\n",
    "        if action == \"right\":\n",
    "            pyautogui.press(\"right\")\n",
    "        elif action == \"left\":\n",
    "            pyautogui.press(\"left\")\n",
    "        elif action == \"up\":\n",
    "            pyautogui.press(\"up\")\n",
    "        elif action == \"down\":\n",
    "            pyautogui.press(\"down\")\n",
    "        elif action == \"space\":\n",
    "            pyautogui.press(\"space\")\n",
    "        return action, current_time  # Update last_action and time\n",
    "    return last_action, last_action_time  # No change\n",
    "\n",
    "def display_text(image, text):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    position = (50, 50)\n",
    "    font_scale = 1\n",
    "    color = (0, 255, 0)\n",
    "    thickness = 2\n",
    "    cv2.putText(image, text, position, font, font_scale, color, thickness)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    last_action = \"\" \n",
    "    last_action_time = time.time() \n",
    " \n",
    "    while cap.isOpened(): \n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break \n",
    "\n",
    "        image , results = mediapipe_detection(frame, hands)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            highlight_finger_points(image, results.multi_hand_landmarks)\n",
    "\n",
    "        action = check_conditions(results, image)\n",
    "        last_action, last_action_time = trigger_key_action(action, last_action, last_action_time)\n",
    "\n",
    "        if action:\n",
    "            display_text(image, action)\n",
    "\n",
    "        cv2.imshow(\"Hand Gesture Detection\", image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd39c12-e6cf-4551-8d9b-80a1d7598434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
